# -*- coding: utf-8 -*-
"""qwen_train_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tyJyicYhY6mrGqY8WXSmQqiA-R2bwAR_
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q transformers datasets accelerate peft bitsandbytes
!pip install -q pandas numpy matplotlib seaborn scikit-learn

!pip install numpy==1.26.4

import os, gc, json, warnings
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer, DataCollatorForSeq2Seq,
    BitsAndBytesConfig, EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from datasets import Dataset as HFDataset

class Config:
    # Model configuration - Optimized for Qwen1.5-1.8B
    MODEL_NAME = "Qwen/Qwen1.5-1.8B"
    MAX_LENGTH = 1648  # As requested
    NUM_EPOCHS = 3

    # Batch sizes - Optimized for Qwen1.5-1.8B (smaller than StableLM 3B)
    TRAIN_BATCH_SIZE = 4    # Can be larger due to smaller model
    EVAL_BATCH_SIZE = 8     # Larger for eval
    GRAD_ACC_STEPS = 2      # 4 * 2 = 8 effective batch size

    # Learning rate - Optimized for Qwen1.5 architecture
    LEARNING_RATE = 2e-4    # Slightly higher for smaller model
    WARMUP_STEPS = 100
    WARMUP_RATIO = 0.03     # Lower warmup ratio for Qwen

    # File paths
    DATASET_DIR = "/content/drive/MyDrive/final_benchmark_dataset"
    TRAIN_PATH = f"{DATASET_DIR}/train.jsonl"
    VAL_PATH = f"{DATASET_DIR}/val.jsonl"
    TEST_PATH = f"{DATASET_DIR}/test.jsonl"

    # Output paths
    OUTPUT_DIR = "/content/drive/MyDrive/qwen_medical_checkpoints"
    LOGS_DIR = "/content/drive/MyDrive/qwen_logs"
    MERGED_DIR = "/content/drive/MyDrive/qwen_merged"

    # LoRA parameters - Optimized for Qwen1.5-1.8B
    LORA_R = 32             # Higher rank for smaller model
    LORA_ALPHA = 64         # 2x rank ratio
    LORA_DROPOUT = 0.1      # Standard dropout

    # Target modules for Qwen1.5 (based on Qwen architecture)
    TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

config = Config()
warnings.filterwarnings('ignore')
tqdm.pandas()

# Check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"‚úÖ Device: {device}")

if torch.cuda.is_available():
    print(f"üî• GPU: {torch.cuda.get_device_name()}")
    print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

def load_jsonl(path):
    """Load JSONL file with error handling"""
    try:
        with open(path, "r", encoding='utf-8') as f:
            data = [json.loads(line.strip()) for line in f if line.strip()]
        print(f"‚úÖ Loaded {len(data)} samples from {path}")
        return data
    except FileNotFoundError:
        print(f"‚ùå File not found: {path}")
        return []
    except json.JSONDecodeError as e:
        print(f"‚ùå JSON decode error: {e}")
        return []

def load_and_prepare_data():
    """Load and prepare all datasets - Full dataset (no subset)"""
    print("üìÇ Loading full datasets...")

    train = load_jsonl(config.TRAIN_PATH)
    val = load_jsonl(config.VAL_PATH)
    test = load_jsonl(config.TEST_PATH)

    if not train or not val or not test:
        raise ValueError("‚ùå One or more datasets are empty!")

    print(f"üìä Full dataset sizes - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}")

    return (
        HFDataset.from_list(train),
        HFDataset.from_list(val),
        HFDataset.from_list(test)
    )

# Load datasets
train_ds, val_ds, test_ds = load_and_prepare_data()

# Load tokenizer
print("üî§ Loading Qwen1.5-1.8B tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, use_fast=True)

# Qwen1.5 specific tokenizer setup
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"‚úÖ Tokenizer loaded. Vocab size: {tokenizer.vocab_size}")

def tokenize_function(examples):
    """
    Optimized tokenization for Qwen1.5-1.8B with instruction masking
    Qwen1.5 uses ChatML format for instruction following
    """
    instructions = examples["instruction"]
    outputs = examples["output"]
    model_inputs = {"input_ids": [], "attention_mask": [], "labels": []}

    for instruction, output in zip(instructions, outputs):
        # Qwen1.5 optimized prompt format using ChatML
        prompt = f"""<|im_start|>system
You are a highly skilled medical AI assistant specialized in generating comprehensive, accurate medical discharge reports. You have extensive knowledge of medical terminology, procedures, and documentation standards.<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
"""

        # Create the full text (prompt + response)
        full_text = prompt + output + "<|im_end|>" + tokenizer.eos_token

        # Tokenize the full text
        tokenized = tokenizer(
            full_text,
            max_length=config.MAX_LENGTH,
            truncation=True,
            padding=False,
            return_tensors=None
        )

        # Tokenize just the prompt to get its length
        prompt_tokens = tokenizer(
            prompt,
            truncation=True,
            padding=False,
            return_tensors=None
        )

        input_ids = tokenized["input_ids"]
        attention_mask = tokenized["attention_mask"]
        labels = input_ids.copy()

        # Mask the prompt part in labels (set to -100)
        prompt_len = len(prompt_tokens["input_ids"])
        if prompt_len < len(labels):
            labels[:prompt_len] = [-100] * prompt_len
        else:
            # If prompt is too long, skip this example
            continue

        # Add to batch
        model_inputs["input_ids"].append(input_ids)
        model_inputs["attention_mask"].append(attention_mask)
        model_inputs["labels"].append(labels)

    return model_inputs

# Tokenize datasets
print("üîÑ Tokenizing datasets...")
tokenized_train = train_ds.map(
    tokenize_function,
    batched=True,
    remove_columns=train_ds.column_names,
    desc="Tokenizing training data"
)

tokenized_val = val_ds.map(
    tokenize_function,
    batched=True,
    remove_columns=val_ds.column_names,
    desc="Tokenizing validation data"
)

print(f"‚úÖ Tokenization complete!")
print(f"üìä Tokenized - Train: {len(tokenized_train)}, Val: {len(tokenized_val)}")

# Add length column for efficient batching
def add_length(example):
    return {"length": len(example["input_ids"])}

tokenized_train = tokenized_train.map(add_length, desc="Adding length column to train")
tokenized_val = tokenized_val.map(add_length, desc="Adding length column to val")

print("‚úÖ Length columns added")

# Load model with quantization - Optimized for Qwen1.5-1.8B
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

print("ü§ñ Loading Qwen1.5-1.8B model...")
model = AutoModelForCausalLM.from_pretrained(
    config.MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    use_cache=False
)

model.config.use_cache = False
model = prepare_model_for_kbit_training(model)

print("üîß Setting up LoRA...")
lora_config = LoraConfig(
    r=config.LORA_R,
    lora_alpha=config.LORA_ALPHA,
    lora_dropout=config.LORA_DROPOUT,
    target_modules=config.TARGET_MODULES,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

print("‚úÖ Model and LoRA setup complete")

# Create output directories
os.makedirs(config.OUTPUT_DIR, exist_ok=True)
os.makedirs(config.LOGS_DIR, exist_ok=True)

training_args = TrainingArguments(
    output_dir=config.OUTPUT_DIR,
    num_train_epochs=config.NUM_EPOCHS,
    per_device_train_batch_size=config.TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=config.EVAL_BATCH_SIZE,
    gradient_accumulation_steps=config.GRAD_ACC_STEPS,
    learning_rate=config.LEARNING_RATE,

    # Optimized scheduler for Qwen1.5
    lr_scheduler_type="cosine",
    warmup_ratio=config.WARMUP_RATIO,
    warmup_steps=config.WARMUP_STEPS,

    # Evaluation and saving
    save_strategy="steps",
    eval_strategy="steps",
    save_steps=100,  # More frequent for smaller model
    eval_steps=100,
    logging_steps=25,

    # Model selection
    load_best_model_at_end=True,
    save_total_limit=3,  # Keep more checkpoints
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    # Performance optimizations for Qwen1.5-1.8B
    fp16=False,  # Use bf16 for better stability
    bf16=True,
    gradient_checkpointing=True,
    dataloader_pin_memory=True,
    dataloader_num_workers=2,

    # Memory optimization
    ddp_find_unused_parameters=False,
    dataloader_drop_last=True,

    # Efficient batching
    group_by_length=True,
    length_column_name="length",

    # Regularization
    weight_decay=0.01,
    max_grad_norm=1.0,

    # Other settings
    prediction_loss_only=True,
    report_to="none",
    logging_dir=config.LOGS_DIR,
    save_safetensors=True,
    remove_unused_columns=False,
    eval_accumulation_steps=1,

    # Stability improvements
    optim="adamw_torch",
    adam_beta1=0.9,
    adam_beta2=0.999,
    adam_epsilon=1e-8,
)

# Data collator
print("üîß Setting up data collator...")
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding="longest",
    label_pad_token_id=-100,
    return_tensors="pt"
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Standard patience for smaller model
)

print("‚úÖ Trainer setup complete")

def start_training():
    """Start training with comprehensive monitoring"""
    gc.collect()
    torch.cuda.empty_cache()

    print("üöÄ Starting Qwen1.5-1.8B Medical Fine-tuning (3 Epochs)...")
    print("=" * 60)
    print(f"ü§ñ Model: {config.MODEL_NAME}")
    print(f"üìä Training samples: {len(tokenized_train)}")
    print(f"üìä Validation samples: {len(tokenized_val)}")
    print(f"üéØ Train batch size: {config.TRAIN_BATCH_SIZE}")
    print(f"üéØ Eval batch size: {config.EVAL_BATCH_SIZE}")
    print(f"üîÑ Gradient accumulation: {config.GRAD_ACC_STEPS}")
    print(f"üî• Effective batch size: {config.TRAIN_BATCH_SIZE * config.GRAD_ACC_STEPS}")
    print(f"üìè Max sequence length: {config.MAX_LENGTH}")
    print(f"üìà Learning rate: {config.LEARNING_RATE}")
    print(f"üîÅ Epochs: {config.NUM_EPOCHS}")
    print(f"üéØ LoRA rank: {config.LORA_R}")
    print(f"üéØ LoRA alpha: {config.LORA_ALPHA}")
    print(f"üéØ LoRA dropout: {config.LORA_DROPOUT}")
    print("=" * 60)

    if torch.cuda.is_available():
        print(f"üî• GPU: {torch.cuda.get_device_name()}")
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"üìä GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB")

    try:
        # Start training
        train_result = trainer.train()

        print("\nüéâ Training completed successfully!")
        print(f"üìâ Final training loss: {train_result.training_loss:.4f}")
        print(f"‚è±Ô∏è Training time: {train_result.training_time:.2f} seconds")

        # Save model
        trainer.save_model()
        print(f"üíæ Model saved to: {config.OUTPUT_DIR}")

        return train_result

    except torch.cuda.OutOfMemoryError:
        print("‚ùå CUDA Out of Memory!")
        print("üí° Try reducing batch size or sequence length")
        print(f"Current: batch_size={config.TRAIN_BATCH_SIZE}, max_length={config.MAX_LENGTH}")
        print(f"Suggested: batch_size=2, grad_acc_steps=4")

        gc.collect()
        torch.cuda.empty_cache()
        return None

    except Exception as e:
        print(f"‚ùå Training error: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# Start training
train_result = start_training()

if train_result is not None:
    print("\nüîÑ Saving final model...")
    trainer.save_model(config.OUTPUT_DIR)
    tokenizer.save_pretrained(config.OUTPUT_DIR)
    print(f"‚úÖ Model + tokenizer saved at {config.OUTPUT_DIR}")

    # Merge and save full model
    print("üîÑ Merging LoRA weights...")
    from peft import PeftModel

    # Clear GPU memory first
    del model, trainer
    gc.collect()
    torch.cuda.empty_cache()

    # Load base model for merging
    base_model = AutoModelForCausalLM.from_pretrained(
        config.MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )

    # Load and merge LoRA weights
    merged_model = PeftModel.from_pretrained(base_model, config.OUTPUT_DIR)
    merged_model = merged_model.merge_and_unload()

    # Save merged model
    os.makedirs(config.MERGED_DIR, exist_ok=True)
    merged_model.save_pretrained(config.MERGED_DIR, safe_serialization=True)
    tokenizer.save_pretrained(config.MERGED_DIR)

    print(f"‚úÖ Fully merged model saved to: {config.MERGED_DIR}")

    # Clear memory
    del merged_model, base_model
    gc.collect()
    torch.cuda.empty_cache()

else:
    print("‚ùå Training failed, model not saved")

print("\nüéØ Qwen1.5-1.8B training script complete!")
print("=" * 60)
print("üìä OPTIMIZATIONS FOR QWEN1.5-1.8B:")
print(f"‚Ä¢ Model: StableLM-3B-4e1t ‚Üí Qwen1.5-1.8B")
print(f"‚Ä¢ Train batch size: 2 ‚Üí 4 (smaller model allows larger batches)")
print(f"‚Ä¢ Gradient accumulation: 4 ‚Üí 2 (maintain effective batch size = 8)")
print(f"‚Ä¢ Learning rate: 1e-4 ‚Üí 2e-4 (higher for smaller model)")
print(f"‚Ä¢ Max length: 1648 (as requested)")
print(f"‚Ä¢ LoRA rank: 16 ‚Üí 32 (higher rank for smaller model)")
print(f"‚Ä¢ LoRA alpha: 32 ‚Üí 64 (2x rank ratio)")
print(f"‚Ä¢ Target modules: Updated for Qwen1.5 architecture")
print(f"‚Ä¢ Prompt format: ChatML format optimized for Qwen1.5")
print(f"‚Ä¢ Warmup ratio: 0.05 ‚Üí 0.03 (lower for Qwen)")
print(f"‚Ä¢ Early stopping patience: 5 ‚Üí 3 (standard for smaller model)")
print(f"‚Ä¢ Save steps: 150 ‚Üí 100 (more frequent for smaller model)")
print(f"‚Ä¢ Logging steps: 50 ‚Üí 25 (more frequent monitoring)")
print(f"‚Ä¢ Dataset: Full 2.5k samples (unchanged)")
print(f"‚Ä¢ Epochs: 3 (as requested)")
print("=" * 60)

# === OPTIONAL: Manual save if needed ===
print("\nüíæ Saving trained LoRA model...")
CHECKPOINT_DIR = "/content/drive/MyDrive/final_qwen_medical_checkpoints"
trainer.save_model(CHECKPOINT_DIR)
tokenizer.save_pretrained(CHECKPOINT_DIR)
print(f"‚úÖ LoRA checkpoint saved to: {CHECKPOINT_DIR}")

# === OPTIONAL: Manual merge if needed ===
print("\nüîÑ Manual merge and save...")
import gc, os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Configuration for manual merge
BASE_MODEL = "Qwen/Qwen1.5-1.8B"
LORA_DIR = CHECKPOINT_DIR  # Use the checkpoint we just saved
MERGED_DIR = "/content/drive/MyDrive/qwen_final_merged"

# Clear memory before loading
gc.collect()
torch.cuda.empty_cache()

# Load base model
print("üì¶ Loading base Qwen1.5-1.8B model...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# Load LoRA weights
print("üîó Loading LoRA adapter...")
lora_model = PeftModel.from_pretrained(base_model, LORA_DIR)

# Merge LoRA into base model
print("üîÑ Merging LoRA into base model...")
merged_model = lora_model.merge_and_unload()

# Save merged model
print("üíæ Saving merged model to disk...")
os.makedirs(MERGED_DIR, exist_ok=True)
merged_model.save_pretrained(MERGED_DIR, safe_serialization=True)

# Save tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
tokenizer.save_pretrained(MERGED_DIR)

print(f"‚úÖ Final merged model saved to: {MERGED_DIR}")
print("üéâ Qwen1.5-1.8B medical fine-tuning complete!")