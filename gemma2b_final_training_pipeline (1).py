# -*- coding: utf-8 -*-
"""gemma2b_final_training_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_jYfkZOlI6ZDRO6WaS5u-3NfxB4Lj7XS
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q transformers datasets accelerate peft bitsandbytes
!pip install -q pandas numpy matplotlib seaborn scikit-learn

!pip install numpy==1.26.4

import os, gc, json, warnings
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# Transformers + PEFT
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer, DataCollatorForSeq2Seq,
    BitsAndBytesConfig, EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from datasets import Dataset as HFDataset

class Config:
    # Model configuration - Optimized for Gemma 2B
    MODEL_NAME = "google/gemma-2b"
    MAX_LENGTH = 1648  # Kept as requested
    NUM_EPOCHS = 3     # Kept as requested

    # Batch sizes - Optimized for Gemma 2B on T4
    TRAIN_BATCH_SIZE = 2   # Smaller due to larger model
    EVAL_BATCH_SIZE = 2    # Smaller due to larger model
    GRAD_ACC_STEPS = 4     # Increased to maintain effective batch size

    # Learning rate - Optimized for Gemma
    LEARNING_RATE = 1e-4   # Conservative for larger model
    WARMUP_STEPS = 100     # More warmup for stability
    WARMUP_RATIO = 0.03    # Lower warmup ratio

    # File paths
    DATASET_DIR = "/content/drive/MyDrive/final_benchmark_dataset"
    TRAIN_PATH = f"{DATASET_DIR}/train.jsonl"
    VAL_PATH = f"{DATASET_DIR}/val.jsonl"
    TEST_PATH = f"{DATASET_DIR}/test.jsonl"

    # Output paths
    OUTPUT_DIR = "/content/drive/MyDrive/FINAL_TRIAL1_gemma_2b_medical_checkpoints"
    LOGS_DIR = "/content/drive/MyDrive/FINAL_TRIAL1_gemma_2b_logs"
    MERGED_DIR = "/content/drive/MyDrive/FINAL_TRIAL1_gemma_2b_merged"

    # LoRA parameters - Optimized for Gemma 2B
    LORA_R = 64           # Higher rank for larger model
    LORA_ALPHA = 128      # Higher alpha for better adaptation
    LORA_DROPOUT = 0.05   # Lower dropout for stability

    # Target modules for Gemma 2B
    TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

config = Config()
warnings.filterwarnings('ignore')
tqdm.pandas()

# Check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"‚úÖ Device: {device}")

if torch.cuda.is_available():
    print(f"üî• GPU: {torch.cuda.get_device_name()}")
    print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

def load_jsonl(path):
    """Load JSONL file with error handling"""
    try:
        with open(path, "r", encoding='utf-8') as f:
            data = [json.loads(line.strip()) for line in f if line.strip()]
        print(f"‚úÖ Loaded {len(data)} samples from {path}")
        return data
    except FileNotFoundError:
        print(f"‚ùå File not found: {path}")
        return []
    except json.JSONDecodeError as e:
        print(f"‚ùå JSON decode error: {e}")
        return []

from huggingface_hub import login
login()

def load_and_prepare_data():
    """Load and prepare all datasets - Full dataset (no subset)"""
    print("üìÇ Loading full datasets...")

    train = load_jsonl(config.TRAIN_PATH)
    val = load_jsonl(config.VAL_PATH)
    test = load_jsonl(config.TEST_PATH)

    if not train or not val or not test:
        raise ValueError("‚ùå One or more datasets are empty!")

    print(f"üìä Full dataset sizes - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}")

    return (
        HFDataset.from_list(train),
        HFDataset.from_list(val),
        HFDataset.from_list(test)
    )

# Load datasets
train_ds, val_ds, test_ds = load_and_prepare_data()

# Load tokenizer
print("üî§ Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, use_fast=True)
# Gemma uses a different pad token setup
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
print(f"‚úÖ Tokenizer loaded. Vocab size: {tokenizer.vocab_size}")

def tokenize_function(examples):
    """
    Optimized tokenization for Gemma 2B with instruction masking
    """
    instructions = examples["instruction"]
    outputs = examples["output"]
    model_inputs = {"input_ids": [], "attention_mask": [], "labels": []}

    for instruction, output in zip(instructions, outputs):
        # Create the prompt using Gemma's chat format with proper medical system prompt
        prompt = f"<bos><start_of_turn>system\nYou are a medical expert assistant specialized in generating comprehensive medical discharge reports based on patient data.<end_of_turn>\n<start_of_turn>user\n{instruction}<end_of_turn>\n<start_of_turn>model\n"

        # Create the full text (prompt + response)
        full_text = prompt + output + "<end_of_turn>"

        # Tokenize the full text
        tokenized = tokenizer(
            full_text,
            max_length=config.MAX_LENGTH,
            truncation=True,
            padding=False,
            return_tensors=None
        )

        # Tokenize just the prompt to get its length
        prompt_tokens = tokenizer(
            prompt,
            truncation=True,
            padding=False,
            return_tensors=None
        )

        input_ids = tokenized["input_ids"]
        attention_mask = tokenized["attention_mask"]
        labels = input_ids.copy()

        # Mask the prompt part in labels (set to -100)
        prompt_len = len(prompt_tokens["input_ids"])
        labels[:prompt_len] = [-100] * prompt_len

        # Add to batch
        model_inputs["input_ids"].append(input_ids)
        model_inputs["attention_mask"].append(attention_mask)
        model_inputs["labels"].append(labels)

    return model_inputs

# Tokenize datasets
print("üîÑ Tokenizing datasets...")
tokenized_train = train_ds.map(
    tokenize_function,
    batched=True,
    remove_columns=train_ds.column_names,
    desc="Tokenizing training data"
)

tokenized_val = val_ds.map(
    tokenize_function,
    batched=True,
    remove_columns=val_ds.column_names,
    desc="Tokenizing validation data"
)

print(f"‚úÖ Tokenization complete!")
print(f"üìä Tokenized - Train: {len(tokenized_train)}, Val: {len(tokenized_val)}")

# Add length column for efficient batching
def add_length(example):
    return {"length": len(example["input_ids"])}

tokenized_train = tokenized_train.map(add_length, desc="Adding length column to train")
tokenized_val = tokenized_val.map(add_length, desc="Adding length column to val")

print("‚úÖ Length columns added")

# Load model with quantization - Optimized for Gemma 2B
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

print("ü§ñ Loading Gemma 2B model...")
model = AutoModelForCausalLM.from_pretrained(
    config.MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    attn_implementation="eager"  # For better compatibility
)

model.config.use_cache = False
model = prepare_model_for_kbit_training(model)

print("üîß Setting up LoRA for Gemma 2B...")
lora_config = LoraConfig(
    r=config.LORA_R,
    lora_alpha=config.LORA_ALPHA,
    lora_dropout=config.LORA_DROPOUT,
    target_modules=config.TARGET_MODULES,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

print("‚úÖ Gemma 2B model and LoRA setup complete")

# Create output directories
os.makedirs(config.OUTPUT_DIR, exist_ok=True)
os.makedirs(config.LOGS_DIR, exist_ok=True)

training_args = TrainingArguments(
    output_dir=config.OUTPUT_DIR,
    num_train_epochs=config.NUM_EPOCHS,
    per_device_train_batch_size=config.TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=config.EVAL_BATCH_SIZE,
    gradient_accumulation_steps=config.GRAD_ACC_STEPS,
    learning_rate=config.LEARNING_RATE,

    # Optimized scheduler for Gemma
    lr_scheduler_type="cosine",
    warmup_ratio=config.WARMUP_RATIO,
    warmup_steps=config.WARMUP_STEPS,

    # Evaluation and saving
    save_strategy="steps",
    eval_strategy="steps",
    save_steps=150,     # Less frequent for larger model
    eval_steps=150,
    logging_steps=50,

    # Model selection
    load_best_model_at_end=True,
    save_total_limit=2,  # Keep more checkpoints for larger model
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    # Performance optimizations for Gemma 2B
    bf16=True,           # Use bf16 instead of fp16 for Gemma
    gradient_checkpointing=True,
    dataloader_pin_memory=False,
    dataloader_num_workers=0,

    # Efficient batching
    group_by_length=True,
    length_column_name="length",

    # Regularization - More conservative for larger model
    weight_decay=0.01,
    max_grad_norm=0.5,   # Lower gradient clipping

    # Other settings
    prediction_loss_only=True,
    report_to="none",
    logging_dir=config.LOGS_DIR,
    save_safetensors=True,
    remove_unused_columns=False,
    eval_accumulation_steps=1,

    # Memory optimization
    dataloader_persistent_workers=False,
    skip_memory_metrics=True,
)

print("üîß Setting up data collator...")
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding="longest",
    label_pad_token_id=-100,
    return_tensors="pt"
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # More patience for larger model
)

print("‚úÖ Trainer setup complete")

def start_training():
    """Start training with comprehensive monitoring"""
    gc.collect()
    torch.cuda.empty_cache()

    print("üöÄ Starting Gemma 2B Medical Fine-tuning (3 Epochs)...")
    print("=" * 50)
    print(f"üìä Training samples: {len(tokenized_train)}")
    print(f"üìä Validation samples: {len(tokenized_val)}")
    print(f"üéØ Batch size: {config.TRAIN_BATCH_SIZE}")
    print(f"üîÑ Gradient accumulation: {config.GRAD_ACC_STEPS}")
    print(f"üî• Effective batch size: {config.TRAIN_BATCH_SIZE * config.GRAD_ACC_STEPS}")
    print(f"üìè Max sequence length: {config.MAX_LENGTH}")
    print(f"üìà Learning rate: {config.LEARNING_RATE}")
    print(f"üîÅ Epochs: {config.NUM_EPOCHS}")
    print(f"üéØ LoRA rank: {config.LORA_R}")
    print(f"üéØ LoRA alpha: {config.LORA_ALPHA}")
    print("=" * 50)

    if torch.cuda.is_available():
        print(f"üî• GPU: {torch.cuda.get_device_name()}")
        allocated = torch.cuda.memory_allocated() / 1024**3
        print(f"üìä GPU Memory Allocated: {allocated:.2f} GB")

    try:
        # Start training
        train_result = trainer.train()

        print("\nüéâ Training completed successfully!")
        print(f"üìâ Final training loss: {train_result.training_loss:.4f}")
        print(f"‚è±Ô∏è Training time: {train_result.training_time:.2f} seconds")

        # Save model
        trainer.save_model()
        print(f"üíæ Model saved to: {config.OUTPUT_DIR}")

        return train_result

    except torch.cuda.OutOfMemoryError:
        print("‚ùå CUDA Out of Memory!")
        print("üí° Try reducing batch size or gradient accumulation steps")
        print(f"Current: batch_size={config.TRAIN_BATCH_SIZE}, grad_acc={config.GRAD_ACC_STEPS}")

        gc.collect()
        torch.cuda.empty_cache()
        return None

    except Exception as e:
        print(f"‚ùå Training error: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# Start training
train_result = start_training()

if train_result is not None:
    print("\nüîÑ Saving final model...")
    trainer.save_model(config.OUTPUT_DIR)
    tokenizer.save_pretrained(config.OUTPUT_DIR)
    print(f"‚úÖ Model + tokenizer saved at {config.OUTPUT_DIR}")

    # Merge and save full model
    print("üîÑ Merging LoRA weights...")
    from peft import PeftModel

    # Load base model
    base_model = AutoModelForCausalLM.from_pretrained(
        config.MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )

    # Load and merge LoRA weights
    merged_model = PeftModel.from_pretrained(base_model, config.OUTPUT_DIR)
    merged_model = merged_model.merge_and_unload()

    # Save merged model
    os.makedirs(config.MERGED_DIR, exist_ok=True)
    merged_model.save_pretrained(config.MERGED_DIR, safe_serialization=True)
    tokenizer.save_pretrained(config.MERGED_DIR)

    print(f"‚úÖ Fully merged model saved to: {config.MERGED_DIR}")

    # Clear memory
    del merged_model, base_model
    gc.collect()
    torch.cuda.empty_cache()

else:
    print("‚ùå Training failed, model not saved")

print("\nüéØ Gemma 2B training script complete!")
print("=" * 50)
print("üìä GEMMA 2B OPTIMIZATIONS:")
print(f"‚Ä¢ Model: TinyLlama 1.1B ‚Üí Gemma 2B")
print(f"‚Ä¢ Train batch size: 8 ‚Üí 2 (larger model)")
print(f"‚Ä¢ Eval batch size: 8 ‚Üí 2 (larger model)")
print(f"‚Ä¢ Gradient accumulation: 1 ‚Üí 4 (maintain effective batch)")
print(f"‚Ä¢ Learning rate: 2e-4 ‚Üí 1e-4 (more conservative)")
print(f"‚Ä¢ Max length: 1024 ‚Üí 1648 (as requested)")
print(f"‚Ä¢ LoRA rank: 32 ‚Üí 64 (higher for larger model)")
print(f"‚Ä¢ LoRA alpha: 64 ‚Üí 128 (higher adaptation)")
print(f"‚Ä¢ LoRA dropout: 0.1 ‚Üí 0.05 (lower for stability)")
print(f"‚Ä¢ Warmup steps: 50 ‚Üí 100 (more warmup)")
print(f"‚Ä¢ Warmup ratio: 0.1 ‚Üí 0.03 (adjusted)")
print(f"‚Ä¢ Save/eval steps: 100 ‚Üí 150 (less frequent)")
print(f"‚Ä¢ Max grad norm: 1.0 ‚Üí 0.5 (lower clipping)")
print(f"‚Ä¢ Precision: fp16 ‚Üí bf16 (better for Gemma)")
print(f"‚Ä¢ Chat format: TinyLlama ‚Üí Gemma format")
print(f"‚Ä¢ Early stopping patience: 3 ‚Üí 5 (more patience)")
print(f"‚Ä¢ Epochs: 3 (as requested)")
print(f"‚Ä¢ Token size: 1648 (as requested)")
print("=" * 50)

# Save model manually
trainer.save_model("/content/drive/MyDrive/fr_gemma2b_final_checkpoint_manual")

# Save tokenizer
tokenizer.save_pretrained("/content/drive/MyDrive/fr_gemma2b_final_checkpoint_manual")

print("‚úÖ Manual checkpoint saved successfully!")

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# Update these paths
BASE_MODEL_PATH = "google/gemma-2b"  # or your base model dir if local
LORA_ADAPTER_PATH = "/content/drive/MyDrive/fr_gemma2b_final_checkpoint_manual"  # your LoRA adapter folder
MERGED_SAVE_PATH = "/content/drive/MyDrive/fr_fr_gemma2b_merged_model"

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Merge LoRA
model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
model = model.merge_and_unload()

# Save merged model
model.save_pretrained(MERGED_SAVE_PATH)

from shutil import copyfile

tokenizer_files = [
    "tokenizer.model",
    "tokenizer_config.json",
    "tokenizer.json",
    "special_tokens_map.json"
]

for file in tokenizer_files:
    src = f"{LORA_ADAPTER_PATH}/{file}"
    dst = f"{MERGED_SAVE_PATH}/{file}"
    try:
        copyfile(src, dst)
        print(f"Copied {file}")
    except:
        print(f"‚ö†Ô∏è {file} not found, skipped.")